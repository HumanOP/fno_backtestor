{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef5e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "class TradeManager:\n",
    "    \"\"\"\n",
    "    Manages the execution of trades and trade book maintenance.\n",
    "    \n",
    "    Responsible for:\n",
    "    - Placing new trade orders\n",
    "    - Closing open positions\n",
    "    - Tracking active trades\n",
    "    - Building and maintaining the trade book\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize the TradeManager with an empty tradebook.\"\"\"\n",
    "        print(\"TradeManager initialized\")\n",
    "        self.tradebook: List[Dict[str, Any]] = []\n",
    "        self.tradebook_built: bool = False\n",
    "        \n",
    "    def place_order(self, entry_data: Dict[str, Any]) -> None:\n",
    "        \"\"\"\n",
    "        Place a new trade order and add it to the tradebook.\n",
    "        \n",
    "        Args:\n",
    "            entry_data: Dictionary containing trade entry details\n",
    "        \"\"\"\n",
    "        trade = {\n",
    "            'strategy_id': entry_data['strategy_id'],\n",
    "            'position_id': entry_data['position_id'],\n",
    "            'leg_id': entry_data['leg_id'],\n",
    "            'symbol': entry_data['symbol'],\n",
    "            'entry_date': entry_data['entry_date'],\n",
    "            'entry_time': entry_data['entry_time'],\n",
    "            'exit_date': None,\n",
    "            'exit_time': None,\n",
    "            'entry_price': entry_data['entry_price'],\n",
    "            'exit_price': None,\n",
    "            'qty': entry_data['qty'],\n",
    "            'entry_type': entry_data['entry_type'],\n",
    "            'entry_spot': entry_data['entry_spot'],\n",
    "            'exit_spot': None,\n",
    "            'stop_loss': entry_data['stop_loss'],\n",
    "            'take_profit': entry_data['take_profit'],\n",
    "            'entry_reason': entry_data['entry_reason'],\n",
    "            'exit_reason': None,\n",
    "            'status': 'open',\n",
    "        }\n",
    "        self.tradebook.append(trade)\n",
    "        \n",
    "    def square_off(self, trade_index: int, exit_data: Dict[str, Any]) -> None:\n",
    "        \"\"\"\n",
    "        Close an open position.\n",
    "        \n",
    "        Args:\n",
    "            trade_index: Index of the trade in tradebook\n",
    "            exit_data: Dictionary containing trade exit details\n",
    "        \"\"\"\n",
    "        self.tradebook[trade_index]['status'] = 'closed'\n",
    "        self.tradebook[trade_index]['exit_date'] = exit_data[\"exit_date\"]\n",
    "        self.tradebook[trade_index]['exit_time'] = exit_data[\"exit_time\"]\n",
    "        self.tradebook[trade_index]['exit_price'] = exit_data[\"exit_price\"]\n",
    "        self.tradebook[trade_index]['exit_spot'] = exit_data[\"exit_spot\"]\n",
    "        self.tradebook[trade_index]['exit_reason'] = exit_data[\"exit_reason\"]\n",
    "    \n",
    "    def active_trades(self) -> List[Tuple[int, Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Return all active trades.\n",
    "        \n",
    "        Returns:\n",
    "            List of tuples containing trade index and trade details\n",
    "        \"\"\"\n",
    "        return [(index, trade) for index, trade in enumerate(self.tradebook) if trade['status'] == 'open']\n",
    "\n",
    "    def build_tradebook(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert tradebook to DataFrame and add calculated columns.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame containing processed tradebook\n",
    "        \"\"\"\n",
    "        # If tradebook is already built, return it\n",
    "        if self.tradebook_built and isinstance(self.tradebook, pd.DataFrame) and not self.tradebook.empty:\n",
    "            print(\"Returning existing tradebook DataFrame\")\n",
    "            return self.tradebook\n",
    "\n",
    "        # Check if tradebook is empty (list or DataFrame)\n",
    "        if isinstance(self.tradebook, list) and not self.tradebook:\n",
    "            print(\"Tradebook is empty. No trades to build.\")\n",
    "            return pd.DataFrame()\n",
    "        elif isinstance(self.tradebook, pd.DataFrame) and self.tradebook.empty:\n",
    "            print(\"Tradebook DataFrame is empty. No trades to build.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Convert list to DataFrame if necessary\n",
    "        if isinstance(self.tradebook, list):\n",
    "            df = pd.DataFrame(self.tradebook)\n",
    "        else:\n",
    "            df = self.tradebook.copy()  # Work on a copy to avoid modifying the original\n",
    "\n",
    "        # Log raw tradebook for debugging\n",
    "        expected_cols = ['symbol', 'entry_date', 'exit_date', 'entry_time', 'exit_time']\n",
    "        if all(col in df.columns for col in expected_cols):\n",
    "            print(\"Raw tradebook before processing:\")\n",
    "            print(df[expected_cols].head())\n",
    "        else:\n",
    "            print(f\"Missing expected columns: {[col for col in expected_cols if col not in df.columns]}\")\n",
    "            print(\"Available columns:\", df.columns.tolist())\n",
    "\n",
    "        # Parse expiry from symbol\n",
    "        def parse_expiry(symbol):\n",
    "            if not isinstance(symbol, str) or len(symbol) < 14:\n",
    "                print(f\"Invalid symbol format: {symbol}\")\n",
    "                return pd.NaT\n",
    "            try:\n",
    "                return datetime.datetime.strptime(symbol[-14:-7], \"%d%b%y\")\n",
    "            except (ValueError, TypeError) as e:\n",
    "                print(f\"Error parsing expiry from symbol {symbol}: {e}\")\n",
    "                return pd.NaT\n",
    "\n",
    "        df[\"expiry\"] = pd.to_datetime(df[\"symbol\"].apply(parse_expiry), errors='coerce')\n",
    "        df[\"instrument_type\"] = df[\"symbol\"].apply(lambda x: x[-2:] if isinstance(x, str) else None)\n",
    "        df[\"strike\"] = df[\"symbol\"].apply(lambda x: x[-7:-2] if isinstance(x, str) else None)\n",
    "\n",
    "        # Convert entry_date and exit_date to datetime\n",
    "        df[\"entry_date\"] = pd.to_datetime(df[\"entry_date\"], errors='coerce')\n",
    "        closed_trades = df['status'] == 'closed'\n",
    "        df.loc[closed_trades, \"exit_date\"] = pd.to_datetime(df.loc[closed_trades, \"exit_date\"], errors='coerce')\n",
    "\n",
    "        # Debug: Check dtypes and nulls\n",
    "        print(f\"entry_date null count: {df['entry_date'].isna().sum()}\")\n",
    "        print(f\"exit_date null count (closed trades): {df.loc[closed_trades, 'exit_date'].isna().sum()}\")\n",
    "        print(f\"expiry null count: {df['expiry'].isna().sum()}\")\n",
    "\n",
    "        if df['entry_date'].isna().any():\n",
    "            print(\"Rows with null entry_date:\")\n",
    "            print(df[df['entry_date'].isna()][['symbol', 'entry_date', 'entry_time']])\n",
    "        if df.loc[closed_trades, 'exit_date'].isna().any():\n",
    "            print(\"Rows with null exit_date (closed trades):\")\n",
    "            print(df.loc[closed_trades & df['exit_date'].isna()][['symbol', 'exit_date', 'exit_time']])\n",
    "        if df['expiry'].isna().any():\n",
    "            print(\"Rows with null expiry:\")\n",
    "            print(df[df['expiry'].isna()][['symbol', 'expiry']])\n",
    "\n",
    "        # Compute entry/exit datetimes\n",
    "        df[\"entry_datetime\"] = df[\"entry_date\"] + pd.to_timedelta(df[\"entry_time\"].astype(str), errors='coerce')\n",
    "        df.loc[closed_trades, \"exit_datetime\"] = (\n",
    "            df.loc[closed_trades, \"exit_date\"] + \n",
    "            pd.to_timedelta(df.loc[closed_trades, \"exit_time\"].astype(str), errors='coerce')\n",
    "        )\n",
    "\n",
    "        # Compute PnL for closed trades\n",
    "        df[\"pnl\"] = df.apply(\n",
    "            lambda row: (\n",
    "                row[\"exit_price\"] - row[\"entry_price\"] if row[\"entry_type\"] == \"BUY\" \n",
    "                else row[\"entry_price\"] - row[\"exit_price\"]\n",
    "            ) if row[\"status\"] == \"closed\" and pd.notna(row[\"exit_price\"]) and pd.notna(row[\"entry_price\"]) else None,\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        self.tradebook = df\n",
    "        self.tradebook_built = True\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c074062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "iv = {}\n",
    "time_arr = {}\n",
    "data = None\n",
    "\n",
    "def calculate_performance(trader):\n",
    "    trades = trader.build_tradebook()\n",
    "    if trades.empty:\n",
    "        return 0.0\n",
    "    # Filter for closed trades (non-null exit_price)\n",
    "    closed_trades = trades[trades['exit_price'].notna()]\n",
    "    if closed_trades.empty:\n",
    "        return 0.0\n",
    "    # Calculate per-trade profits\n",
    "    profits = (closed_trades['pnl'] * closed_trades['qty']).to_numpy()\n",
    "    returns = pd.Series(profits)\n",
    "    if len(returns) < 2:\n",
    "        return 0.0  # Need at least 2 trades for standard deviation\n",
    "    # Calculate annualized Sharpe Ratio (assuming daily returns)\n",
    "    sharpe_ratio = returns.mean() / returns.std() * (252 ** 0.5)\n",
    "    return sharpe_ratio if not pd.isna(sharpe_ratio) else 0.0\n",
    "\n",
    "def parse_table_name(table_name):\n",
    "    \"\"\"Convert table name like 'YYYY-MM-DD' or 'nifty_YYYY_MM_DD' to datetime.date.\"\"\"\n",
    "    if not isinstance(table_name, str):\n",
    "        print(f\"Invalid table name: {table_name} (type: {type(table_name)})\")\n",
    "        return None\n",
    "    \n",
    "    # Try parsing 'YYYY-MM-DD' format\n",
    "    if re.match(r'\\d{4}-\\d{2}-\\d{2}', table_name):\n",
    "        try:\n",
    "            return datetime.datetime.strptime(table_name, \"%Y-%m-%d\").date()\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing date from {table_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Try parsing 'nifty_YYYY_MM_DD' format\n",
    "    match = re.match(r'nifty_(\\d{4})_(\\d{2})_(\\d{2})', table_name)\n",
    "    if match:\n",
    "        year, month, day = match.groups()\n",
    "        print(f\"Parsed {table_name}: year={year}, month={month}, day={day}\")  # Debug\n",
    "        try:\n",
    "            year, month, day = int(year), int(month), int(day)\n",
    "            # Validate date components\n",
    "            if not (1 <= month <= 12):\n",
    "                print(f\"Invalid month in {table_name}: {month}\")\n",
    "                return None\n",
    "            if not (1 <= day <= 31):  # Basic check; could use calendar.monthrange for precision\n",
    "                print(f\"Invalid day in {table_name}: {day}\")\n",
    "                return None\n",
    "            return datetime.date(year, month, day)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing date from {table_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"Cannot parse table name: {table_name}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def backtest(legs, iv_slope_thresolds, duckdb, trader, dates):\n",
    "    global iv, time_arr, data\n",
    "    signal = 0\n",
    "    position_id = 1\n",
    "    counter = 1\n",
    "    iv_slope = 0\n",
    "    \n",
    "    # Convert table names to dates\n",
    "    parsed_dates = [parse_table_name(date_str) for date_str in dates]\n",
    "    if any(date is None for date in parsed_dates):\n",
    "        print(\"Warning: Some table names could not be parsed into dates\")\n",
    "    parsed_dates = [date for date in parsed_dates if date is not None]\n",
    "    if not parsed_dates:\n",
    "        print(\"Error: No valid dates parsed from table names\")\n",
    "        return 0.0\n",
    "\n",
    "    for date in parsed_dates:\n",
    "        date_str = f\"nifty_{date.strftime('%Y_%m_%d')}\"\n",
    "        start_time = time.time()\n",
    "        print(f\"Processing table: {date_str} at {counter}\")\n",
    "        counter += 1\n",
    "        try:\n",
    "            data_df = duckdb.execute(f\"SELECT * FROM {date_str} ORDER BY timestamp\").fetchdf()\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying table {date_str}: {e}\")\n",
    "            continue\n",
    "        time_to_expiry = sorted(data_df[\"Time_to_expiry\"].unique())\n",
    "        data_df.set_index(\"timestamp\", inplace=True)\n",
    "        ticker_map = {ticker: group for ticker, group in data_df.groupby(\"ticker\", sort=False)}\n",
    "        empty_df_template = data_df.iloc[0:0]\n",
    "\n",
    "        for leg in legs.values():\n",
    "            try:\n",
    "                valid_tte = min(tte for tte in time_to_expiry if any(lower <= tte <= upper for lower, upper in [leg[\"expiry_range\"]]))\n",
    "                leg[\"expiry\"] = data_df.loc[data_df[\"Time_to_expiry\"] == valid_tte, \"expiry_date\"].iloc[0]\n",
    "                if not isinstance(leg[\"expiry\"], (datetime.datetime, pd.Timestamp, datetime.date)):\n",
    "                    print(f\"Invalid expiry {leg['expiry']} (type: {type(leg['expiry'])}) for leg {leg['type']}\")\n",
    "                    leg[\"expiry\"] = pd.NaT\n",
    "            except (ValueError, IndexError) as e:\n",
    "                print(f\"Error setting expiry for leg {leg['type']}: {e}\")\n",
    "                continue\n",
    "\n",
    "        spot = data_df[[\"spot_price\"]][~data_df.index.duplicated(keep=\"first\")]\n",
    "        for row in spot.itertuples():\n",
    "            if row.spot_price is None or pd.isna(row.spot_price):\n",
    "                continue\n",
    "            atm = round(row.spot_price / 50) * 50\n",
    "\n",
    "            for leg in legs.values():\n",
    "                if leg[\"target_strike\"] == \"ATM\":\n",
    "                    leg[\"strike\"] = float(atm)\n",
    "                try:\n",
    "                    if pd.isna(leg[\"expiry\"]):\n",
    "                        raise ValueError(\"Expiry is NaT\")\n",
    "                    contract = f\"NIFTY{pd.Timestamp(leg['expiry']).strftime('%d%b%y').upper()}{int(leg['strike'])}{leg['type']}\"\n",
    "                except (ValueError, TypeError) as e:\n",
    "                    print(f\"Error forming contract for leg {leg['type']}: {e}\")\n",
    "                    continue\n",
    "                leg[\"contract\"] = contract\n",
    "                subset_df = ticker_map.get(contract, empty_df_template)\n",
    "                data = subset_df\n",
    "                avl_time = subset_df.index.asof(row.Index) if not subset_df.empty else None\n",
    "                leg[\"data\"] = subset_df.loc[avl_time] if not pd.isna(avl_time) else None\n",
    "\n",
    "            missing_legs = [leg[\"contract\"] for leg in legs.values() if leg[\"data\"] is None]\n",
    "            if missing_legs:\n",
    "                continue\n",
    "\n",
    "            if (pd.Timestamp(\"15:29:00\").time() <= pd.Timestamp(row.Index).time() <= pd.Timestamp(\"15:30:00\").time()):\n",
    "                try:\n",
    "                    iv_slope = math.log((legs[\"leg1\"][\"data\"][\"iv\"] + legs[\"leg2\"][\"data\"][\"iv\"]) / (legs[\"leg3\"][\"data\"][\"iv\"] + legs[\"leg4\"][\"data\"][\"iv\"]), 10)\n",
    "                    iv[row.Index] = (iv_slope, row.spot_price)\n",
    "                except (ValueError, TypeError) as e:\n",
    "                    print(f\"Error calculating iv_slope: {e}\")\n",
    "                    continue\n",
    "\n",
    "            new_signal = (\n",
    "                (iv_slope > iv_slope_thresolds[\"upper_gamma\"]) * 3 +\n",
    "                (iv_slope_thresolds[\"upper_gamma\"] >= iv_slope > iv_slope_thresolds[\"upper_buffer\"]) * 2 +\n",
    "                (iv_slope_thresolds[\"upper_buffer\"] >= iv_slope > 0) * 1 +\n",
    "                (0 >= iv_slope > iv_slope_thresolds[\"lower_buffer\"]) * -1 +\n",
    "                (iv_slope_thresolds[\"lower_buffer\"] >= iv_slope > iv_slope_thresolds[\"lower_gamma\"]) * -2 +\n",
    "                (iv_slope_thresolds[\"lower_gamma\"] >= iv_slope) * -3\n",
    "            )\n",
    "\n",
    "            active_trades = trader.active_trades()\n",
    "            if (not active_trades) and (pd.Timestamp(row.Index).time() < pd.Timestamp(\"15:00:00\").time()):\n",
    "                if new_signal == -2 or new_signal == 2:\n",
    "                    continue\n",
    "                elif new_signal == 1:\n",
    "                    entry_type_dict = {'weekly': 'BUY', 'monthly': 'SELL'}\n",
    "                elif new_signal == -1:\n",
    "                    entry_type_dict = {'weekly': 'SELL', 'monthly': 'BUY'}\n",
    "                elif new_signal == -3 or new_signal == 3:\n",
    "                    entry_type_dict = {'weekly': 'BUY', 'monthly': None}\n",
    "\n",
    "                for leg_id, leg in legs.items():\n",
    "                    entry_type = entry_type_dict.get(leg[\"expiry_type\"])\n",
    "                    if entry_type is None:\n",
    "                        continue\n",
    "                    entry_data = {\n",
    "                        'strategy_id': 'strat1',\n",
    "                        'position_id': position_id,\n",
    "                        'leg_id': leg_id,\n",
    "                        'symbol': leg[\"contract\"],\n",
    "                        'entry_date': pd.Timestamp(row.Index).date(),\n",
    "                        'entry_time': pd.Timestamp(row.Index).time(),\n",
    "                        'entry_price': leg[\"data\"][\"close\"],\n",
    "                        'qty': 1,\n",
    "                        'entry_type': entry_type,\n",
    "                        'entry_spot': row.spot_price,\n",
    "                        'stop_loss': None,\n",
    "                        'take_profit': None,\n",
    "                        'entry_reason': f'{new_signal} signal entry',\n",
    "                    }\n",
    "                    trader.place_order(entry_data)\n",
    "                position_id += 1\n",
    "            else:\n",
    "                near_expiry = None\n",
    "                for index, trade in active_trades:\n",
    "                    try:\n",
    "                        expiry = datetime.datetime.strptime(trade[\"symbol\"][-14:-7], \"%d%b%y\").date()\n",
    "                        near_expiry = expiry if near_expiry is None else min(near_expiry, expiry)\n",
    "                    except (ValueError, TypeError) as e:\n",
    "                        print(f\"Error parsing expiry from trade symbol {trade['symbol']}: {e}\")\n",
    "                        continue\n",
    "                exit_reason = (\n",
    "                    \"End of Data reached\" if ((date == parsed_dates[-1]) and (pd.Timestamp(row.Index).time() > pd.Timestamp(\"15:00:00\").time())) else\n",
    "                    \"Near Expiry reached\" if (pd.Timestamp(row.Index).date() == near_expiry) else\n",
    "                    \"Signal changed\" if (signal != new_signal) else\n",
    "                    None\n",
    "                )\n",
    "                if exit_reason:\n",
    "                    for index, trade in active_trades:\n",
    "                        contract = trade[\"symbol\"]\n",
    "                        subset_df = ticker_map.get(contract, empty_df_template)\n",
    "                        subset_df = subset_df.loc[:row.Index]\n",
    "                        if not subset_df.empty:\n",
    "                            close_price = subset_df.iloc[-1][\"close\"]\n",
    "                        else:\n",
    "                            close_price = trade[\"entry_price\"]\n",
    "                        exit_data = {\n",
    "                            'exit_date': pd.Timestamp(row.Index).date(),\n",
    "                            'exit_time': pd.Timestamp(row.Index).time(),\n",
    "                            'exit_price': close_price,\n",
    "                            'exit_spot': row.spot_price,\n",
    "                            'exit_reason': exit_reason,\n",
    "                        }\n",
    "                        trader.square_off(index, exit_data)\n",
    "\n",
    "                if signal == new_signal:\n",
    "                    leg_strike = legs[\"leg2\"][\"strike\"]\n",
    "                    if (row.spot_price * 0.99) <= leg_strike <= (row.spot_price * 1.01):\n",
    "                        continue\n",
    "                    else:\n",
    "                        for leg_id, leg in legs.items():\n",
    "                            entry_type = entry_type_dict.get(leg[\"expiry_type\"])\n",
    "                            if entry_type is None:\n",
    "                                continue\n",
    "                            entry_data = {\n",
    "                                'strategy_id': 'strat1',\n",
    "                                'position_id': position_id,\n",
    "                                'leg_id': leg_id,\n",
    "                                'symbol': leg[\"contract\"],\n",
    "                                'entry_date': pd.Timestamp(row.Index).date(),\n",
    "                                'entry_time': pd.Timestamp(row.Index).time(),\n",
    "                                'entry_price': leg[\"data\"][\"close\"],\n",
    "                                'qty': 1,\n",
    "                                'entry_type': entry_type,\n",
    "                                'entry_spot': row.spot_price,\n",
    "                                'stop_loss': None,\n",
    "                                'take_profit': None,\n",
    "                                'entry_reason': f'Adjustment Calendar',\n",
    "                            }\n",
    "                            trader.place_order(entry_data)\n",
    "                        position_id += 1\n",
    "\n",
    "            signal = new_signal\n",
    "        time_arr[date_str] = time.time() - start_time\n",
    "\n",
    "    # Force-close any remaining open trades\n",
    "    active_trades = trader.active_trades()\n",
    "    if active_trades:\n",
    "        print(f\"Force-closing {len(active_trades)} open trades\")\n",
    "        last_date = parsed_dates[-1] if parsed_dates else datetime.date.today()\n",
    "        for index, trade in active_trades:\n",
    "            contract = trade[\"symbol\"]\n",
    "            subset_df = ticker_map.get(contract, empty_df_template)\n",
    "            close_price = subset_df.iloc[-1][\"close\"] if not subset_df.empty else trade[\"entry_price\"]\n",
    "            exit_data = {\n",
    "                'exit_date': last_date,\n",
    "                'exit_time': pd.Timestamp(\"15:30:00\").time(),\n",
    "                'exit_price': close_price,\n",
    "                'exit_spot': spot.iloc[-1][\"spot_price\"] if not spot.empty else trade[\"entry_spot\"],\n",
    "                'exit_reason': \"End of backtest\",\n",
    "            }\n",
    "            trader.square_off(index, exit_data)\n",
    "\n",
    "    return calculate_performance(trader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from openpyxl import Workbook\n",
    "from itertools import product\n",
    "import itertools\n",
    "import datetime\n",
    "from openpyxl.drawing.image import Image\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "class HyperParameterOptimizer:\n",
    "    def __init__(self, legs: Dict, conn: duckdb.DuckDBPyConnection, dates: pd.Series):\n",
    "        \"\"\"\n",
    "        Initialize the HyperParameterOptimizer.\n",
    "\n",
    "        Args:\n",
    "            legs: Dictionary of trade legs\n",
    "            conn: DuckDB connection object\n",
    "            dates: Series of dates for backtesting\n",
    "        \"\"\"\n",
    "        self.legs = legs\n",
    "        self.conn = conn\n",
    "        self.dates = dates\n",
    "        self.trader = TradeManager()\n",
    "\n",
    "    def optimize(self, hyperparameter_grid: Dict[str, List[float]], maximize: str = 'Sharpe Ratio', \n",
    "                 method: str = 'grid', max_tries: Optional[int] = None, constraint: Optional[callable] = None) -> Tuple[Dict, float, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Optimize hyperparameters using grid search.\n",
    "\n",
    "        Args:\n",
    "            hyperparameter_grid: Dictionary of hyperparameters with their possible values\n",
    "            maximize: Metric to maximize ('Sharpe Ratio')\n",
    "            method: Optimization method ('grid' only for now)\n",
    "            max_tries: Maximum number of parameter combinations to try\n",
    "            constraint: Function to filter admissible parameter combinations\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (best parameters, best Sharpe ratio, results DataFrame)\n",
    "        \"\"\"\n",
    "        if method != 'grid':\n",
    "            raise ValueError(\"Only 'grid' method is supported in this implementation\")\n",
    "\n",
    "        # Generate all parameter combinations\n",
    "        param_keys = list(hyperparameter_grid.keys())\n",
    "        param_values = [hyperparameter_grid[key] for key in param_keys]\n",
    "        param_combinations = [dict(zip(param_keys, combo)) for combo in product(*param_values)]\n",
    "\n",
    "        # Apply constraint if provided\n",
    "        if constraint:\n",
    "            param_combinations = [params for params in param_combinations if constraint(params)]\n",
    "        \n",
    "        # Limit combinations if max_tries is specified\n",
    "        if max_tries and max_tries < len(param_combinations):\n",
    "            param_combinations = param_combinations[:max_tries]\n",
    "\n",
    "        if not param_combinations:\n",
    "            raise ValueError(\"No admissible parameter combinations to test\")\n",
    "\n",
    "        # Initialize results storage\n",
    "        results = []\n",
    "        best_sharpe = -np.inf\n",
    "        best_params = None\n",
    "\n",
    "        # Run backtest for each parameter combination\n",
    "        for params in param_combinations:\n",
    "            print(f\"Testing parameters: {params}\")\n",
    "            self.trader = TradeManager()  # Reset trader for each run\n",
    "            sharpe_ratio = backtest(self.legs, params, self.conn, self.trader, self.dates)\n",
    "            results.append({**params, 'Sharpe Ratio': sharpe_ratio})\n",
    "\n",
    "            print(f\"Sharpe ratio : {sharpe_ratio}\")\n",
    "            \n",
    "            if sharpe_ratio > best_sharpe:\n",
    "                best_sharpe = sharpe_ratio\n",
    "                best_params = params\n",
    "\n",
    "        # Convert results to DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Generate heatmaps\n",
    "        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        output_directory_path = os.path.join(os.getcwd(), f'hyperparameter_optimizer_output_{timestamp}')\n",
    "        os.makedirs(output_directory_path, exist_ok=True)\n",
    "        sheet_path = os.path.join(output_directory_path, 'summary.xlsx')\n",
    "        \n",
    "        self.generate_heatmaps(results_df, sheet_path, output_directory_path)\n",
    "        self.delete_png_files(output_directory_path)\n",
    "\n",
    "        return best_params, best_sharpe, results_df\n",
    "\n",
    "    def generate_heatmaps(self, results_df: pd.DataFrame, output_file: str, png_directory: str) -> None:\n",
    "        \"\"\"\n",
    "        Generate heatmaps for the Sharpe Ratio and save to Excel.\n",
    "\n",
    "        Args:\n",
    "            results_df: DataFrame containing optimization results\n",
    "            output_file: Path to save the Excel file\n",
    "            png_directory: Directory to save temporary heatmap images\n",
    "        \"\"\"\n",
    "        workbook = Workbook()\n",
    "        sheet = workbook.create_sheet(title=\"Sharpe Ratio\")\n",
    "\n",
    "        # Write results to sheet\n",
    "        for row_idx, row in enumerate(\n",
    "            [results_df.columns.tolist()] + results_df.values.tolist(), start=1\n",
    "        ):\n",
    "            for col_idx, value in enumerate(row, start=1):\n",
    "                sheet.cell(row=row_idx, column=col_idx, value=value)\n",
    "\n",
    "        # Add heatmaps\n",
    "        start_row = len(results_df) + 3\n",
    "        parameter_columns = [col for col in results_df.columns if col != \"Sharpe Ratio\"]\n",
    "        parameter_pairs = list(itertools.combinations(parameter_columns, 2))\n",
    "\n",
    "        for param_x, param_y in parameter_pairs:\n",
    "            grouped = (\n",
    "                results_df.groupby([param_x, param_y])[\"Sharpe Ratio\"]\n",
    "                .mean()\n",
    "                .unstack(fill_value=np.nan)\n",
    "            )\n",
    "\n",
    "            cmap = plt.cm.viridis\n",
    "            cmap = cmap.copy()\n",
    "            cmap.set_bad(color=\"white\")\n",
    "\n",
    "            plt.figure(figsize=(5, 4))\n",
    "            ax = plt.gca()\n",
    "            heatmap = ax.imshow(\n",
    "                grouped.values, cmap=cmap, aspect=\"auto\", interpolation=\"nearest\"\n",
    "            )\n",
    "            ax.set_xticks(np.arange(len(grouped.columns)))\n",
    "            ax.set_yticks(np.arange(len(grouped.index)))\n",
    "            ax.set_xticklabels(grouped.columns, rotation=45)\n",
    "            ax.set_yticklabels(grouped.index)\n",
    "            ax.set_xlabel(param_y)\n",
    "            ax.set_ylabel(param_x)\n",
    "            ax.set_title(f\"Heatmap of Sharpe Ratio: {param_x} vs {param_y}\")\n",
    "            plt.colorbar(heatmap, ax=ax)\n",
    "\n",
    "            image_file = f\"{param_x}_{param_y}_Sharpe_Ratio_heatmap.png\"\n",
    "            image_file = os.path.join(png_directory, image_file)\n",
    "            plt.savefig(image_file, bbox_inches=\"tight\", dpi=150)\n",
    "            plt.close()\n",
    "\n",
    "            img = Image(image_file)\n",
    "            img.anchor = f\"A{start_row}\"\n",
    "            sheet.add_image(img)\n",
    "            start_row += 30\n",
    "\n",
    "        if \"Sheet\" in workbook.sheetnames:\n",
    "            workbook.remove(workbook[\"Sheet\"])\n",
    "        workbook.save(output_file)\n",
    "        print(f\"Heatmaps saved to {output_file}\")\n",
    "\n",
    "    def delete_png_files(self, png_directory: str) -> None:\n",
    "        \"\"\"\n",
    "        Delete temporary PNG files.\n",
    "\n",
    "        Args:\n",
    "            png_directory: Directory containing PNG files\n",
    "        \"\"\"\n",
    "        for filename in os.listdir(png_directory):\n",
    "            if filename.endswith(\".png\"):\n",
    "                file_path = os.path.join(png_directory, filename)\n",
    "                os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f270881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WalkForwardOptimizer:\n",
    "    \"\"\"\n",
    "    Performs walk-forward optimization for trading strategy hyperparameters using HyperParameterOptimizer.\n",
    "\n",
    "    Attributes:\n",
    "        legs (dict): Trading legs configuration.\n",
    "        hyperparameter_grid (dict): Grid of hyperparameters to optimize.\n",
    "        duckdb: DuckDB connection object.\n",
    "        dates (pd.Series): Series of dates for backtesting.\n",
    "        in_sample_ratio (float): Proportion of data for in-sample testing.\n",
    "        out_sample_ratio (float): Proportion of data for out-of-sample testing.\n",
    "    \"\"\"\n",
    "    def __init__(self, legs: Dict, hyperparameter_grid: Dict[str, List[float]], duckdb: duckdb.DuckDBPyConnection, \n",
    "                 dates: pd.Series, in_sample_ratio: float = 0.6/4, out_sample_ratio: float = 0.2/4):\n",
    "        \"\"\"Initialize the WalkForwardOptimizer with necessary parameters.\"\"\"\n",
    "        self.legs = legs\n",
    "        self.hyperparameter_grid = hyperparameter_grid\n",
    "        self.duckdb = duckdb\n",
    "        self.dates = dates.tolist()\n",
    "        self.in_sample_ratio = in_sample_ratio\n",
    "        self.out_sample_ratio = out_sample_ratio\n",
    "        self.total_dates = len(self.dates)\n",
    "        self.in_sample_size = int(self.total_dates * in_sample_ratio)\n",
    "        self.out_sample_size = int(self.total_dates * out_sample_ratio)\n",
    "\n",
    "        if self.in_sample_size + self.out_sample_size > self.total_dates:\n",
    "            raise ValueError(\"In-sample + out-sample size exceeds total dates\")\n",
    "\n",
    "    def optimize(self) -> Tuple[Dict, float, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Perform walk-forward optimization using HyperParameterOptimizer.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (best_params, avg_out_sample_performance, results)\n",
    "                - best_params: Best hyperparameters from in-sample optimization.\n",
    "                - avg_out_sample_performance: Average Sharpe ratio from out-sample tests.\n",
    "                - results: List of dictionaries with window, in-sample, and out-sample results.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        out_sample_performances = []\n",
    "\n",
    "        # Define the constraint function for hyperparameters\n",
    "        def constraint(params: Dict) -> bool:\n",
    "            return params[\"upper_gamma\"] > params[\"upper_buffer\"] > 0 > params[\"lower_buffer\"] > params[\"lower_gamma\"]\n",
    "\n",
    "        for start in range(0, self.total_dates - self.in_sample_size - self.out_sample_size + 1, self.out_sample_size):\n",
    "            in_sample_dates = self.dates[start:start + self.in_sample_size]\n",
    "            out_sample_dates = self.dates[start + self.in_sample_size:start + self.in_sample_size + self.out_sample_size]\n",
    "\n",
    "            print(f\"Processing window: In-sample {in_sample_dates[0]} to {in_sample_dates[-1]}, \"\n",
    "                  f\"Out-sample {out_sample_dates[0]} to {out_sample_dates[-1]}\")\n",
    "\n",
    "            # Initialize HyperParameterOptimizer for in-sample data\n",
    "            optimizer = HyperParameterOptimizer(self.legs, self.duckdb, pd.Series(in_sample_dates))\n",
    "            \n",
    "            try:\n",
    "                # Perform grid search on in-sample data\n",
    "                best_in_sample_params, best_in_sample_performance, results_df = optimizer.optimize(\n",
    "                    hyperparameter_grid=self.hyperparameter_grid,\n",
    "                    maximize='Sharpe Ratio',\n",
    "                    method='grid',\n",
    "                    constraint=constraint\n",
    "                )\n",
    "                \n",
    "                # Debug: Log in-sample results\n",
    "                print(f\"In-sample best params: {best_in_sample_params}, Sharpe Ratio: {best_in_sample_performance}\")\n",
    "                results_df.to_csv(f'in_sample_results_window_{start}.csv')\n",
    "\n",
    "            except (duckdb.ConnectionException, ValueError) as e:\n",
    "                print(f\"Error during in-sample optimization: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Test best parameters on out-sample data\n",
    "            trader = TradeManager()\n",
    "            try:\n",
    "                out_sample_performance = backtest(self.legs, best_in_sample_params, self.duckdb, trader, pd.Series(out_sample_dates))\n",
    "                \n",
    "                # Debug: Log out-sample tradebook\n",
    "                tradebook = trader.build_tradebook()\n",
    "                print(f\"Out-sample: {len(tradebook)} trades, Sharpe Ratio: {out_sample_performance}\")\n",
    "                tradebook.to_csv(f'out_sample_tradebook_window_{start}.csv')\n",
    "\n",
    "            except duckdb.ConnectionException as e:\n",
    "                print(f\"Connection error during out-sample testing: {e}\")\n",
    "                continue\n",
    "\n",
    "            results.append({\n",
    "                'window': (in_sample_dates[0], out_sample_dates[-1]),\n",
    "                'in_sample_params': best_in_sample_params,\n",
    "                'in_sample_performance': best_in_sample_performance,\n",
    "                'out_sample_performance': out_sample_performance\n",
    "            })\n",
    "            out_sample_performances.append(out_sample_performance)\n",
    "\n",
    "        if not results:\n",
    "            raise ValueError(\"No valid results from walk-forward optimization\")\n",
    "\n",
    "        avg_out_sample_performance = sum(out_sample_performances) / len(out_sample_performances)\n",
    "        best_result = max(results, key=lambda x: x['out_sample_performance'])\n",
    "        best_params = best_result['in_sample_params']\n",
    "\n",
    "        # Save results to JSON\n",
    "        wfo_output = {\n",
    "            'best_params': best_params,\n",
    "            'avg_out_sample_sharpe_ratio': avg_out_sample_performance,\n",
    "            'results': [\n",
    "                {\n",
    "                    'window': (str(result['window'][0]), str(result['window'][1])),\n",
    "                    'in_sample_params': result['in_sample_params'],\n",
    "                    'in_sample_sharpe_ratio': result['in_sample_performance'],\n",
    "                    'out_sample_sharpe_ratio': result['out_sample_performance']\n",
    "                } for result in results\n",
    "            ]\n",
    "        }\n",
    "        with open('wfo_results.json', 'w') as f:\n",
    "            json.dump(wfo_output, f, indent=4)\n",
    "\n",
    "        return best_params, avg_out_sample_performance, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6311ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    db_path = \"nifty_1min_desiquant.duckdb\"\n",
    "    conn = None\n",
    "    try:\n",
    "        if not os.path.exists(db_path):\n",
    "            raise FileNotFoundError(f\"Database file '{db_path}' not found in {os.getcwd()}\")\n",
    "        conn = duckdb.connect(db_path)\n",
    "        \n",
    "        # Fetch table names\n",
    "        table_names = conn.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'main'\").fetchdf()\n",
    "        if table_names.empty:\n",
    "            raise ValueError(\"No tables found in the database\")\n",
    "        \n",
    "        # Debug: Print table names\n",
    "        print(\"Table names DataFrame columns:\", table_names.columns.tolist())\n",
    "        print(\"First few rows of table_names:\")\n",
    "        print(table_names.head())\n",
    "        \n",
    "        # Filter table names for the specified range\n",
    "        table_names = table_names[90:703]\n",
    "        \n",
    "        # Filter table names using regex pattern\n",
    "        pattern = re.compile(r'nifty_\\d{4}_\\d{2}_\\d{2}')\n",
    "        table_names = table_names[table_names['table_name'].str.match(pattern)]\n",
    "        if table_names.empty:\n",
    "            raise ValueError(\"No dates available in the specified range [90:703]\")\n",
    "        \n",
    "        # Convert table names to dates\n",
    "        dates = pd.Series([parse_table_name(name) for name in table_names['table_name']], index=table_names.index)\n",
    "        dates = dates.dropna()\n",
    "        if dates.empty:\n",
    "            raise ValueError(\"No valid dates parsed from table names\")\n",
    "        dates = dates.apply(lambda x: x.strftime('%Y-%m-%d'))  # Convert to string format for consistency\n",
    "        \n",
    "        legs = {\n",
    "            'leg1': {'type': 'CE', 'expiry_type': 'weekly', 'expiry_range': [12, 20], 'target_strike': 'ATM', 'stop_loss': None, 'take_profit': None},\n",
    "            'leg2': {'type': 'PE', 'expiry_type': 'weekly', 'expiry_range': [12, 20], 'target_strike': 'ATM', 'stop_loss': None, 'take_profit': None},\n",
    "            'leg3': {'type': 'CE', 'expiry_type': 'monthly', 'expiry_range': [26, 34], 'target_strike': 'ATM', 'stop_loss': None, 'take_profit': None},\n",
    "            'leg4': {'type': 'PE', 'expiry_type': 'monthly', 'expiry_range': [26, 34], 'target_strike': 'ATM', 'stop_loss': None, 'take_profit': None}\n",
    "        }\n",
    "\n",
    "        hyperparameter_grid = {\n",
    "            \"upper_gamma\": [0.0, 0.02, 0.04],\n",
    "            \"upper_buffer\": [-0.02, 0.00, 0.02],\n",
    "            \"lower_buffer\": [-0.03, -0.06, -0.09],\n",
    "            \"lower_gamma\": [-0.05, -0.08, -0.11]\n",
    "        }\n",
    "        \n",
    "        # Perform Walk-Forward Optimization\n",
    "        print(\"Running Walk-Forward Optimization...\")\n",
    "        optimizer = WalkForwardOptimizer(legs, hyperparameter_grid, conn, dates, in_sample_ratio=0.6/4, out_sample_ratio=0.2/4)\n",
    "        best_params, avg_performance, wfo_results = optimizer.optimize()\n",
    "        \n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        print(f\"Average out-of-sample Sharpe Ratio: {avg_performance}\")\n",
    "        print(\"WFO results saved to 'wfo_results.json'\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Please ensure the database file exists in the current working directory or provide the correct path.\")\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied accessing '{db_path}'\")\n",
    "        print(\"Please check file permissions and ensure you have read/write access.\")\n",
    "    except duckdb.IOException as e:\n",
    "        print(f\"Error: Failed to open database: {e}\")\n",
    "        print(\"The database file may be corrupted or locked. Try restoring from a backup or checking for open connections.\")\n",
    "    except duckdb.ConnectionException as e:\n",
    "        print(f\"Connection error: {e}\")\n",
    "        print(\"The database connection was closed unexpectedly. Check for connection timeouts or resource issues.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            try:\n",
    "                conn.close()\n",
    "                print(\"Database connection closed successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error closing connection: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backtester",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
